{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import TFBertModel, BertTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langkah-langkah untuk Membangun Sistem Rekomendasi Resep dengan Jaringan Siamese\n",
    "Notebook ini bertujuan untuk membuat sistem rekomendasi resep menggunakan pendekatan jaringan Siamese. Berikut adalah tahapan-tahapan yang akan kita lakukan dalam proses ini:\n",
    "\n",
    "- Memuat dan Memproses Dataset: Menggabungkan kolom-kolom penting seperti kategori, judul, dan bahan untuk menjadi satu teks yang nantinya akan diproses oleh model BERT.\n",
    "\n",
    "- Membuat Embedding BERT: Menggunakan model BERT yang sudah dilatih sebelumnya untuk menghasilkan embedding dari setiap resep, yang akan merepresentasikan setiap resep dalam bentuk vektor.\n",
    "\n",
    "- Membuat Pasangan Positif dan Negatif: Berdasarkan kategori, kita membuat pasangan positif (kategori sama) dan negatif (kategori berbeda) sebagai data latih untuk model.\n",
    "\n",
    "- Mendefinisikan dan Melatih Jaringan Siamese: Membangun dan melatih jaringan Siamese dengan menggunakan pasangan positif dan negatif. Model ini dilatih menggunakan contrastive loss untuk mengoptimalkan jarak antara resep yang mirip dan yang tidak mirip.\n",
    "\n",
    "- Menyimpan Model dan Embedding: Setelah pelatihan selesai, kita simpan model dan embedding resep sehingga bisa digunakan lagi tanpa harus melatih model atau menghitung ulang embedding setiap kali.\n",
    "\n",
    "Setelah proses selesai, sistem ini dapat memberikan rekomendasi resep yang mirip berdasarkan resep-resep yang sudah disimpan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df = pd.read_csv('full_format_recipes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama-tama, kita muat dataset dan menggabungkan beberapa kolom seperti categories, title, dan ingredients menjadi satu teks. Ini membantu kita membuat representasi teks yang lebih lengkap untuk setiap resep, sehingga BERT bisa menghasilkan embedding yang lebih baik.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(row):\n",
    "    categories = ' '.join(eval(row['categories'])) if pd.notnull(row['categories']) else ''\n",
    "    ingredients = ' '.join(eval(row['ingredients'])) if pd.notnull(row['ingredients']) else ''\n",
    "    return f\"{categories}. {ingredients}\"\n",
    "\n",
    "# Menghapus Fitur Data ['title'] untuk membuat model similiarity lebih spread out\n",
    "\n",
    "recipes_df['bert_input_text'] = recipes_df.apply(preprocess_text, axis=1)\n",
    "texts = recipes_df['bert_input_text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membuat Embedding BERT\n",
    "Setelah teks untuk setiap resep siap, kita menggunakan model BERT untuk menghasilkan embedding. Embedding ini akan digunakan sebagai dasar untuk melatih jaringan siamese, yang akan belajar mengenali kemiripan antar resep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings = []\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "    outputs = bert_model(inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  \n",
    "    bert_embeddings.append(cls_embedding.numpy().flatten()) \n",
    "\n",
    "bert_embeddings = np.array(bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df['embedding'] = list(bert_embeddings)\n",
    "embeddings = recipes_df['embedding']\n",
    "embeddings = np.array(list(recipes_df['embedding'])).reshape(-1, 768, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membuat Pasangan Positif dan Negatif\n",
    "Pada tahap ini, kita membuat pasangan positif dan negatif. Pasangan positif berisi resep-resep yang memiliki kategori serupa, sementara pasangan negatif memiliki kategori yang berbeda. Dengan pasangan-pasangan ini, model bisa belajar membedakan antara resep yang mirip dan tidak mirip.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_category(categories1, categories2):\n",
    "    set1 = set(eval(categories1)) if isinstance(categories1, str) else set(categories1)\n",
    "    set2 = set(eval(categories2)) if isinstance(categories2, str) else set(categories2)\n",
    "    return len(set1.intersection(set2)) > 0\n",
    "\n",
    "def generate_pairs(recipes_df, embeddings, num_pairs=1000):\n",
    "    positive_pairs, negative_pairs = [], []\n",
    "    \n",
    "    for _ in range(num_pairs):\n",
    "        idx1 = np.random.randint(len(recipes_df))\n",
    "        \n",
    "        positive_candidates = [idx for idx in recipes_df.index if idx != idx1 and common_category(\n",
    "            recipes_df.iloc[idx1]['categories'], recipes_df.iloc[idx]['categories'])]\n",
    "        \n",
    "        if positive_candidates:\n",
    "            idx2 = np.random.choice(positive_candidates)\n",
    "            positive_pairs.append([embeddings[idx1], embeddings[idx2]])\n",
    "\n",
    "        negative_candidates = [idx for idx in recipes_df.index if idx != idx1 and not common_category(\n",
    "            recipes_df.iloc[idx1]['categories'], recipes_df.iloc[idx]['categories'])]\n",
    "        \n",
    "        if negative_candidates:\n",
    "            idx3 = np.random.choice(negative_candidates)\n",
    "            negative_pairs.append([embeddings[idx1], embeddings[idx3]])\n",
    "\n",
    "    return np.array(positive_pairs), np.array(negative_pairs)\n",
    "\n",
    "positive_pairs, negative_pairs = generate_pairs(recipes_df, embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mendefinisikan dan Melatih Model Jaringan Siamese\n",
    "Di bagian ini, kita membangun jaringan Siamese. Jaringan ini memiliki arsitektur berbagi bobot (shared network), yang berarti kedua input resep (dalam pasangan positif atau negatif) akan diproses melalui jaringan yang sama. Model ini dilatih dengan menggunakan contrastive loss sehingga embedding dari resep yang mirip akan lebih dekat, sedangkan yang tidak mirip akan semakin berjauhan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    positive_loss = y_true * tf.square(y_pred)\n",
    "    negative_loss = (1 - y_true) * tf.square(tf.maximum(margin - y_pred, 0))\n",
    "    return tf.reduce_mean(positive_loss + negative_loss)\n",
    "\n",
    "input_shape = (768,)  \n",
    "input_a = layers.Input(shape=input_shape)\n",
    "input_b = layers.Input(shape=input_shape)\n",
    "\n",
    "shared_network = tf.keras.Sequential([\n",
    "    layers.InputLayer(shape=(768,)),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu')\n",
    "])\n",
    "processed_a = shared_network(input_a)\n",
    "processed_b = shared_network(input_b)\n",
    "distance = layers.Lambda(lambda embeddings: tf.norm(embeddings[0] - embeddings[1], axis=1, keepdims=True))([processed_a, processed_b])\n",
    "siamese_model = Model(inputs=[input_a, input_b], outputs=distance)\n",
    "siamese_model.compile(optimizer='adam', loss=contrastive_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f3a4e7e090>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_model.fit([positive_pairs[:, 0], positive_pairs[:, 1]], np.ones(len(positive_pairs)),\n",
    "                  epochs=10, batch_size=32)\n",
    "\n",
    "siamese_model.fit([negative_pairs[:, 0], negative_pairs[:, 1]], np.zeros(len(negative_pairs)),\n",
    "                  epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menyimpan Model dan Embedding\n",
    "Setelah selesai melatih model, kita simpan model yang sudah terlatih dan embedding yang telah dihasilkan. Dengan begitu, kita dapat menggunakan model ini untuk rekomendasi tanpa harus menghitung ulang embedding atau melatih ulang model setiap kali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "siamese_model.save(\"siamese_model_up.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.save(\"siamese_model_up.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df.to_csv(\"recipes_with_final_embeddings_up.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"bert_embeddings_up.npz\", bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"pairs_data_up.npz\", positive_pairs=positive_pairs, negative_pairs=negative_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = np.load(\"pairs_data_up.npz\")\n",
    "positive_pairs = loaded_data['positive_pairs']\n",
    "negative_pairs = loaded_data['negative_pairs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive pairs sample: [[[[-0.76791894]\n",
      "   [-0.2520941 ]\n",
      "   [-0.13335492]\n",
      "   ...\n",
      "   [-0.88982743]\n",
      "   [-0.00443599]\n",
      "   [ 0.04973048]]\n",
      "\n",
      "  [[-0.55208814]\n",
      "   [ 0.05981527]\n",
      "   [ 0.25554323]\n",
      "   ...\n",
      "   [-0.6429797 ]\n",
      "   [-0.05616171]\n",
      "   [ 0.17085686]]]\n",
      "\n",
      "\n",
      " [[[-0.66103214]\n",
      "   [-0.30348742]\n",
      "   [-0.3116385 ]\n",
      "   ...\n",
      "   [-0.19763482]\n",
      "   [-0.01908185]\n",
      "   [ 0.23871489]]\n",
      "\n",
      "  [[-0.57389295]\n",
      "   [-0.09762322]\n",
      "   [ 0.02418999]\n",
      "   ...\n",
      "   [-0.40753457]\n",
      "   [-0.32132417]\n",
      "   [ 0.16687067]]]\n",
      "\n",
      "\n",
      " [[[-0.62901556]\n",
      "   [-0.133837  ]\n",
      "   [ 0.06293941]\n",
      "   ...\n",
      "   [-0.6795049 ]\n",
      "   [ 0.00517223]\n",
      "   [ 0.15844518]]\n",
      "\n",
      "  [[-0.6297413 ]\n",
      "   [-0.1843409 ]\n",
      "   [-0.03472301]\n",
      "   ...\n",
      "   [-0.43930492]\n",
      "   [-0.24457535]\n",
      "   [ 0.06998876]]]\n",
      "\n",
      "\n",
      " [[[-0.66011465]\n",
      "   [ 0.08648633]\n",
      "   [ 0.18873326]\n",
      "   ...\n",
      "   [-0.46646905]\n",
      "   [-0.19052383]\n",
      "   [ 0.18789072]]\n",
      "\n",
      "  [[-0.62304646]\n",
      "   [-0.06366557]\n",
      "   [ 0.23521498]\n",
      "   ...\n",
      "   [-0.6202389 ]\n",
      "   [ 0.04591829]\n",
      "   [ 0.11876895]]]\n",
      "\n",
      "\n",
      " [[[-0.7901331 ]\n",
      "   [-0.19719501]\n",
      "   [ 0.10059106]\n",
      "   ...\n",
      "   [-0.5001638 ]\n",
      "   [-0.04654457]\n",
      "   [ 0.2494964 ]]\n",
      "\n",
      "  [[-0.514445  ]\n",
      "   [ 0.1174162 ]\n",
      "   [ 0.123347  ]\n",
      "   ...\n",
      "   [-0.44187713]\n",
      "   [ 0.03226039]\n",
      "   [ 0.3433079 ]]]]\n",
      "Negative pairs sample: [[[[-0.76791894]\n",
      "   [-0.2520941 ]\n",
      "   [-0.13335492]\n",
      "   ...\n",
      "   [-0.88982743]\n",
      "   [-0.00443599]\n",
      "   [ 0.04973048]]\n",
      "\n",
      "  [[-0.70138884]\n",
      "   [-0.11515817]\n",
      "   [ 0.24900965]\n",
      "   ...\n",
      "   [-0.46368492]\n",
      "   [-0.11153842]\n",
      "   [ 0.4316973 ]]]\n",
      "\n",
      "\n",
      " [[[-0.66103214]\n",
      "   [-0.30348742]\n",
      "   [-0.3116385 ]\n",
      "   ...\n",
      "   [-0.19763482]\n",
      "   [-0.01908185]\n",
      "   [ 0.23871489]]\n",
      "\n",
      "  [[-0.57393765]\n",
      "   [ 0.17609078]\n",
      "   [ 0.16482717]\n",
      "   ...\n",
      "   [-0.4482059 ]\n",
      "   [-0.24167228]\n",
      "   [ 0.32248625]]]\n",
      "\n",
      "\n",
      " [[[-0.62901556]\n",
      "   [-0.133837  ]\n",
      "   [ 0.06293941]\n",
      "   ...\n",
      "   [-0.6795049 ]\n",
      "   [ 0.00517223]\n",
      "   [ 0.15844518]]\n",
      "\n",
      "  [[-0.7701479 ]\n",
      "   [-0.11318696]\n",
      "   [-0.00217382]\n",
      "   ...\n",
      "   [-0.42261603]\n",
      "   [-0.09581962]\n",
      "   [ 0.05039978]]]\n",
      "\n",
      "\n",
      " [[[-0.66011465]\n",
      "   [ 0.08648633]\n",
      "   [ 0.18873326]\n",
      "   ...\n",
      "   [-0.46646905]\n",
      "   [-0.19052383]\n",
      "   [ 0.18789072]]\n",
      "\n",
      "  [[-0.6318749 ]\n",
      "   [-0.11552563]\n",
      "   [ 0.15695533]\n",
      "   ...\n",
      "   [-0.5921691 ]\n",
      "   [-0.24458817]\n",
      "   [ 0.20067814]]]\n",
      "\n",
      "\n",
      " [[[-0.7901331 ]\n",
      "   [-0.19719501]\n",
      "   [ 0.10059106]\n",
      "   ...\n",
      "   [-0.5001638 ]\n",
      "   [-0.04654457]\n",
      "   [ 0.2494964 ]]\n",
      "\n",
      "  [[-0.64583164]\n",
      "   [-0.24054144]\n",
      "   [ 0.3346733 ]\n",
      "   ...\n",
      "   [-0.43542325]\n",
      "   [-0.18580988]\n",
      "   [ 0.8196589 ]]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive pairs sample:\", positive_pairs[:5])\n",
    "print(\"Negative pairs sample:\", negative_pairs[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance for positive pairs: 6.665176\n",
      "Average distance for negative pairs: 7.252311\n"
     ]
    }
   ],
   "source": [
    "# Calculate distances between embeddings in positive pairs\n",
    "pos_distances = [np.linalg.norm(pair[0] - pair[1]) for pair in positive_pairs]\n",
    "neg_distances = [np.linalg.norm(pair[0] - pair[1]) for pair in negative_pairs]\n",
    "\n",
    "print(\"Average distance for positive pairs:\", np.mean(pos_distances))\n",
    "print(\"Average distance for negative pairs:\", np.mean(neg_distances))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
